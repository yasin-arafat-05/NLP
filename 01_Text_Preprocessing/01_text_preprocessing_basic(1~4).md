# Topics:

- What is NLP?
- Real World Applications of NLP.
- Common NLP Tasks
- Approaches in NLP
- NLP Pipeline 
- Text Preparation

<br>

# `#01 What is NLP:`

<br>
Natural language processing is a subfield of linguistics,computer science and artificial intelligence concerned with the interactions between computers and human language, inparticular how to progrma computer to process and analyze large amounts of natural language data.

<br>

`NLP, Natural Language Processing, Natural Language ‡¶π‡¶≤‡ßã ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá, ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶≠‡¶æ‡¶¨‡ßá‡¶∞ ‡¶Ü‡¶¶‡¶æ‡¶®-‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßá ‡•§ Natural Language processing ‡¶π‡¶≤‡ßã linguistics, AI and Comupter Science ‡¶è‡¶∞ part ‡¶Ø‡ßá‡¶á‡¶ü‡¶æ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá computer ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá natural language ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá operate ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü ‡¶∏‡ßá‡¶á‡¶ü‡¶æ ‡¶®‡¶ø‡ßü‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡•§`

<br>

# `#02 Real World Applicaitons:`

<br>

- Contextual Advertisements <br>
- Email Clients - spam filtering,smart reply <br>
- Social Media - removing adult content, opinion mining <br>
- Search Engines <br>
- Chatbots 


<br>

# `#03 Common NLP Tasks:`

<br>

- Text/Document Classification
- Sentiment Analysis
- Information Retrieval **(RAG type application)**.
- Parts of Speech Tagging **(find noun,verb,adjective)**.
- Machine Translation or language translation
- Conversational Agents
- Knowledge Graph **(use in google search)**
- Text summarization
- Text generation or next word predictor
- Speech to Text

<br>

# `#04 Approaches in NLP:`

<br>

- **1. Heuristic Method.**
- **2. Machine learning Based.**
- **3. Deep learning Based.**
<br>

## **1. Heuristic Method: (Rule-Based Approach)**  
### **‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:**  
- ‡¶≠‡¶æ‡¶∑‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶∞‡¶£, ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ (‡¶∞‡ßÅ‡¶≤‡¶∏) ‡¶è‡¶¨‡¶Ç ‡¶Ö‡¶≠‡¶ø‡¶ß‡¶æ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡¶§‡ßã‡•§  
- ‡¶™‡ßç‡¶∞‡ßã‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶¨‡¶ø‡¶¶‡¶¶‡ßá‡¶∞ ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡¶ø ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ ‡¶°‡¶ø‡¶ú‡¶æ‡¶á‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶§‡ßã‡•§  
### **Era:**  
‡ßß‡ßØ‡ß¨‡ß¶‚Äì‡ßß‡ßØ‡ßØ‡ß¶ ‡¶∏‡¶æ‡¶≤ ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ (‡¶Æ‡ßá‡¶∂‡¶ø‡¶® ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡¶ø‡¶Ç ‡¶Ü‡¶∏‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá)‡•§  
### **‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**  
- **ELIZA (‡ßß‡ßØ‡ß¨‡ß¨):** ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü, ‡¶∏‡¶æ‡¶á‡¶ï‡ßã‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø ‡¶∏‡¶ø‡¶Æ‡ßÅ‡¶≤‡ßá‡¶ü ‡¶ï‡¶∞‡¶§ (‡¶Ø‡ßá‡¶Æ‡¶®: "Tell me more about your family")‡•§  
- **‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡¶æ‡¶∞ ‡¶ö‡ßá‡¶ï‡¶æ‡¶∞:** "He go to school" ‚Üí "go" ‡¶ï‡ßá "goes" ‡¶ï‡¶∞‡¶§‡ßá ‡¶¨‡¶≤‡¶§ (‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶∞‡¶£ ‡¶∞‡ßÅ‡¶≤ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö ‡¶ï‡¶∞‡ßá)‡•§  
- **‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶ø‡¶Ç ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö‡¶ø‡¶Ç:** ‡¶Ø‡ßá‡¶Æ‡¶®, "‡¶¢‡¶æ‡¶ï‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶ö‡¶ü‡ßç‡¶ü‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶∏?" ‚Üí "‡¶¢‡¶æ‡¶ï‡¶æ" ‡¶è‡¶¨‡¶Ç "‡¶ö‡¶ü‡ßç‡¶ü‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ" ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö ‡¶ï‡¶∞‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡ßá‡¶Ø‡¶º‡¶æ‡•§  
### **‡¶∏‡ßÄ‡¶Æ‡¶æ‡¶¨‡¶¶‡ßç‡¶ß‡¶§‡¶æ:**  
- ‡¶®‡¶§‡ßÅ‡¶® ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶¨‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶è‡¶≤‡ßá‡¶á ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶§ ‡¶®‡¶æ‡•§  
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶≠‡¶æ‡¶∑‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ ‡¶¨‡¶æ‡¶®‡¶æ‡¶§‡ßá ‡¶π‡¶§‡ßã‡•§  


## **2. Machine learning Based:(Statistical Approach)**  
### **‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:**  
-  Uses statistical models trained on data (e.g., Na√Øve Bayes, SVM, HMMs) for tasks like classification or sequence labeling.
- **Feature Engineering:** ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶® ‡¶π‡¶§‡ßã (‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶™‡¶ú‡¶ø‡¶∂‡¶®, ‡¶™‡¶æ‡¶∞‡ßç‡¶ü-‡¶Ö‡¶´-‡¶∏‡ßç‡¶™‡¶ø‡¶ö)‡•§  
### **Era:** ‡ßß‡ßØ‡ßØ‡ß¶‚Äì‡ß®‡ß¶‡ßß‡ß¶ ‡¶∏‡¶æ‡¶≤ (‡¶°‡¶ø‡¶™ ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡¶ø‡¶Ç ‡¶Ü‡¶∏‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá)‡•§  
### **‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**  
- **Statistical Machine Translation (SMT):**  
  - ‡¶ó‡ßÅ‡¶ó‡¶≤ ‡¶ü‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶∏‡¶≤‡ßá‡¶ü (‡ß®‡ß¶‡ß¶‡ß¨‚Äì‡ß®‡ß¶‡ßß‡ß¨) "Phrase-Based SMT" ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡•§  
  - ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‚Üí ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶Ö‡¶®‡ßÅ‡¶¨‡¶æ‡¶¶‡ßá ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡ßç‡¶Ø‡¶§‡¶æ (Probability) ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶ï‡¶∞‡¶§‡•§  
- **‡¶∏‡ßç‡¶™‡ßç‡¶Ø‡¶æ‡¶Æ ‡¶´‡¶ø‡¶≤‡ßç‡¶ü‡¶æ‡¶∞:** Na√Øve Bayes ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶≤‡¶ó‡¶∞‡¶ø‡¶¶‡¶Æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶á‡¶Æ‡ßá‡¶á‡¶≤ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶æ‡¶á ‡¶ï‡¶∞‡¶æ‡•§  
- **‡¶™‡¶æ‡¶∞‡ßç‡¶ü-‡¶Ö‡¶´-‡¶∏‡ßç‡¶™‡¶ø‡¶ö ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó‡¶ø‡¶Ç:** Hidden Markov Model (HMM) ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶ü‡ßç‡¶Ø‡¶æ‡¶ó (‡¶Ø‡ßá‡¶Æ‡¶®: noun, verb) ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡•§  


## **3. Deep learning Based:(Neural Approach)**  
### **‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:**  
- ‡¶®‡¶ø‡¶â‡¶∞‡¶æ‡¶≤ ‡¶®‡ßá‡¶ü‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶ï (RNN, Transformer) ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Ö‡¶ü‡ßã‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï ‡¶∂‡ßá‡¶ñ‡ßá‡•§  
- **‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶á‡¶û‡ßç‡¶ú‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶Ç** ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶π‡¶Ø‡¶º ‡¶®‡¶æ‚Äî‡¶Æ‡¶°‡ßá‡¶≤ ‡¶®‡¶ø‡¶ú‡ßá ‡¶•‡ßá‡¶ï‡ßá ‡¶∂‡¶ø‡¶ñ‡ßá ‡¶®‡ßá‡¶Ø‡¶º!  
### **Era:**  
‡ß®‡ß¶‡ßß‡ß¶‚Äì‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® (‡¶è‡¶ñ‡¶®‡¶ï‡¶æ‡¶∞ ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶è‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏‡¶° ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø)‡•§  
### **‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**  
- **‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶° ‡¶è‡¶Æ‡ßç‡¶¨‡ßá‡¶°‡¶ø‡¶Ç (Word2Vec, GloVe):**  
  - ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∞‡¶ø‡¶™‡ßç‡¶∞‡ßá‡¶ú‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∂‡¶® (‡¶Ø‡ßá‡¶Æ‡¶®: "‡¶∞‡¶æ‡¶ú‡¶æ - ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ + ‡¶Æ‡¶π‡¶ø‡¶≤‡¶æ = ‡¶∞‡¶æ‡¶®‡¶ø")‡•§  
- **‡¶∏‡¶ø‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶∏-‡¶ü‡ßÅ-‡¶∏‡¶ø‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶∏ (Seq2Seq):**  
  - RNN/LSTM ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Æ‡ßá‡¶∂‡¶ø‡¶® ‡¶ü‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶∏‡¶≤‡ßá‡¶∂‡¶® (‡¶Ø‡ßá‡¶Æ‡¶®: Google Neural MT, ‡ß®‡ß¶‡ßß‡ß¨)‡•§  
- **‡¶ü‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶∏‡¶´‡¶∞‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶°‡ßá‡¶≤ (BERT, GPT):**  
  - ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶ú‡¶ø‡¶™‡¶ø‡¶ü‡¶ø (GPT-‡ß©/‡ß™), ‡¶ó‡ßÅ‡¶ó‡¶≤‡ßá‡¶∞ BERT‚Äî‡¶è‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶ñ‡¶® ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡¶´‡ßÅ‡¶≤‡•§  
### **‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:**  
- ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡ßç‡¶ü ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá (‡¶Ø‡ßá‡¶Æ‡¶®: "‡¶¨‡¶æ‡¶Å‡¶∂" ‡¶¨‡¶≤‡¶§‡ßá ‡¶ó‡¶æ‡¶õ ‡¶®‡¶æ ‡¶¨‡¶æ‡¶Å‡¶∂‡¶ø ‡¶¨‡ßã‡¶ù‡¶æ‡¶ö‡ßç‡¶õ‡ßá?)‡•§  
- ‡¶è‡¶ï‡¶á ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ü‡¶æ‡¶∏‡ßç‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º (‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø-‡¶ü‡¶æ‡¶∏‡ßç‡¶ï ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡¶ø‡¶Ç)‡•§  

<br>

---

---

---

<br>

# `#05 What is NLP Pipeline?`

<br>

NLP is a set of steps followed to build an end to end NLP software.

**NLP software consists of the following steps:**
- Data Acquisition: **(Gather data)**
- Text Preparation: **(Remove emoji,html tag etc.)**
    - Text Cleaning.
    - Basic text preprocessing.
    - Advance text preprocessing.
- Feature Engineering or Text Vectorization.
- Modelling
    - Model Building
    - Evaluation
- Deployment


<br>

# `#06 Text Preparation:`
<br>


![img](../img/roadmap_text.jpeg)
![img](../img/cleaning.jpeg)
![img](../img/basic_text.jpeg)
<br>

### **Why tokenization is important?**
$20 ‡¶è‡¶ñ‡¶æ‡¶®‡ßá, $ ‡¶Ü‡¶∞ 20  ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡•§  <br>
10KM ‡¶è‡¶ñ‡¶æ‡¶®‡ßá, 10 and KM ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡•§  <br>
New-Work ‡¶è‡¶ñ‡¶æ‡¶®‡ßá, 'New-Work'‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá, New Delhi, "New Delhi" ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡•§ <br>

**‡¶ï‡ßá‡¶® tokenization important?**
‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶â‡¶™‡¶∞‡ßá ‡¶Ø‡ßá, example ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡ßá‡¶ñ‡¶≤‡¶æ‡¶Æ ‡•§ ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ø‡¶ñ‡¶® ‡¶è‡¶ï‡¶ü‡¶æ chatbot ‡¶¨‡¶æ nlp related ‡¶ï‡ßã‡¶® ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨‡ßã ‡¶§‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ programmer ‡¶è‡¶∞ knowledge ‡¶è ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡¶Ø‡ßá, ‡¶â‡¶™‡¶∞‡ßá‡¶∞ word ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶á‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡•§ ‡¶®‡¶æ ‡¶π‡¶≤‡ßá, ‡¶Ü‡¶Æ‡¶æ‡¶∞ chatbot ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨‡ßá ‡¶®‡¶æ ‡•§ user ‡¶ï‡¶ø ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡¶õ‡ßá? ‡¶¨‡¶æ ‡¶ï‡ßã‡¶® ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ö‡¶æ‡¶á‡¶õ‡ßá ‡•§ ‡¶¨‡¶æ, ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ø‡¶ñ‡¶® model train ‡¶ï‡¶∞‡¶¨‡ßã ‡¶§‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶§‡ßã resource ‡¶è‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ limitation ‡¶•‡¶æ‡¶ï‡¶¨‡ßá  ‡•§ ‡¶≠‡¶æ‡¶≤‡ßã gpu ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶®‡¶æ, ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ø‡¶¶‡¶ø ‡¶®‡¶ø‡¶ú‡ßá‡¶¶‡ßá‡¶∞ ‡¶á‡¶ö‡ßç‡¶õ‡¶æ ‡¶Æ‡¶§‡ßã token number ‡¶¨‡¶æ‡ßú‡¶æ‡ßü ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶§‡ßã ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ resouce ‡¶®‡¶ø‡ßü‡ßá ‡¶ù‡¶æ‡¶Æ‡ßá‡¶≤‡¶æ ‡¶π‡¶¨‡ßá ‡•§ ‡¶§‡¶æ‡¶á, tokenization ‡¶∏‡¶Æ‡ßü duplicated words, lemmitation ‡¶è‡¶∞ word ‡¶è‡¶∞ ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶≠‡¶ø‡¶®‡ßç‡¶® form like in english we have, preasent,past,futeure, gerund etc. ‡¶§‡¶æ‡¶á, tokenization ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ part NLP application ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡•§ 

### **Sub-Word Tokenization: Used in LLM's:**

`‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ø‡ßá input sentence ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶∏‡ßá‡¶á‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ ‡•§ ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡ß™‡¶ü‡¶æ ‡¶•‡¶æ‡¶ï‡ßá ‡¶Ü‡¶∞ ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶ï‡ßá ‡ß® ‡¶ü‡¶æ sentence ‡¶è merge ‡¶ï‡¶∞‡¶ø ‡¶§‡¶ñ‡¶® ‡¶∏‡ßá‡¶á‡¶ü‡¶æ ‡¶Ü‡¶Æ‡¶∞‡¶æ Sentence Tokenization ‡¶¨‡¶≤‡¶ø ‡•§ ‡¶Ü‡¶∞ ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶Æ‡¶∞‡¶æ word ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶á‡ßü‡ßÄ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶ø ‡¶§‡¶ñ‡¶® ‡¶∏‡ßá‡¶á‡¶ü‡¶æ‡¶ï‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ Word Tokenization ‡¶¨‡¶≤‡¶ø‡•§ ‡¶§‡ßá‡¶Æ‡¶® ‡¶≠‡¶æ‡¶¨‡ßá‡¶É sub-word(‡¶®‡¶ø‡¶ö‡ßá‡¶∞ example), character tokenization ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡•§ ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶®‡¶ø‡¶ú‡ßá‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã, Tokenization technique Define ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø ‡•§  `

**For example:** `the word "unbelievable" might be broken down into the sub-words "un," "believe," and "able." This helps the model process a larger vocabulary of words.`<br><br>

`‡¶∂‡ßÅ‡¶∞‡ßÅ‡¶§‡ßá, openAi ‡¶Ø‡¶ñ‡¶®, character based Tokenization Technique ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá, ‡¶§‡¶ñ‡¶® ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ Corpus ‡¶è unique word ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡•§ ‡¶è‡¶§ ‡¶ó‡ßÅ‡¶≤‡ßã word ‡¶è‡¶∞ Embedding ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶∏‡¶π‡¶ú ‡¶õ‡¶ø‡¶≤ ‡¶®‡¶æ ‡•§ ‡¶è‡¶õ‡¶æ‡ßú‡¶æ ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶Æ‡¶® ‡¶ï‡ßã‡¶® word ‡¶Ø‡ßá‡¶á‡¶ü‡¶æ openAi ‡¶è‡¶∞ traning ‡¶∏‡¶Æ‡ßü ‡¶Ü‡¶∏‡ßá‡¶®‡¶ø ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ, model ‡¶è‡¶∞ inference ‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü‡ßá ‡¶è‡¶∏‡ßá‡¶õ‡ßá ‡¶§‡¶ñ‡¶® ‡¶∏‡ßá‡¶á‡¶ü‡¶æ OOV(out of Vocuvulary) error ‡¶¶‡¶ø‡¶¨‡ßá ‡•§ ‡¶è‡¶ú‡¶®‡ßç‡¶Ø openAi ‡¶§‡¶æ‡¶Å‡¶¶‡ßá‡¶∞ sub-word tokenization technique ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá ‡•§ "university" -> "uni" and "versity." ‡¶è‡¶á‡¶≠‡¶æ‡¶¨‡ßá sub-word tokenization ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶´‡¶≤‡ßá ‡¶ï‡ßã‡¶® ‡¶è‡¶ï ‡¶∏‡¶Æ‡ßü‡ßá "uni" "versity" traning ‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü embedding ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡¶ì‡ßü‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨‡¶®‡¶æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßá‡¶∂‡¶ø ‡¶•‡¶æ‡¶ï‡ßá ‡•§   `

`Some of the popular subword tokenization algorithms are **WordPiece, Byte-Pair Encoding (BPE), Unigram, and SentencePiece.** .BPE is used in language models like GPT-2, RoBERTa, XLM, FlauBERT, etc. A few of these models use space tokenization as the pre-tokenization method while a few use more advanced pre-tokenization methods provided by Moses, spaCY, ftfy.` <br>

- `Sub-word Tokenization Tenique ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá OpenAi -> BPE(Byte-Pair Encoding) ‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá google Sentence Piece ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡•§ `

`** BPE Sub-Word Tokenization ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá  ‡¶™‡ßú‡¶≤‡ßá‡¶ì ‡•§ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá, word ‡¶ï‡ßá subword ‡¶è ‡¶≠‡¶æ‡¶ó ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá openAI, ‡¶è‡¶á Sub-Word Tokenization ‡¶ï‡ßá  ‡¶è‡¶ï‡¶ü‡ßÅ modify ‡¶ï‡¶∞‡ßá‡¶õ‡ßá ‡•§  but, ultemitely, Tokenization ‡¶è‡¶∞ ‡¶ï‡ßã‡¶® ‡¶®‡¶ø‡ßü‡¶Æ ‡¶®‡ßá‡¶á ‡•§ ‡¶è‡¶ï‡¶ü‡¶æ, company ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶§‡¶æ‡¶∞ word ‡¶ï‡ßá tokenization ‡¶ï‡¶∞‡¶¨‡ßá ‡¶∏‡ßá‡¶á‡¶ü‡¶æ ‡¶§‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞ ‡¶ï‡¶∞‡ßá ‡•§ **`

<br>

## `**Byte-Pair Encoding (BPE)**` <br>

BPE is a simple form of data compression algorithm in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur in that data. It was first described in the article ‚ÄúA New Algorithm for Data Compression‚Äù published in 1994. The below example will explain BPE and has been taken from Wikipedia.<br>

`Suppose we have data aaabdaaabac which needs to be encoded (compressed). The byte pair aa occurs most often, so we will replace it with Z as Z does not occur in our data. So we now have ZabdZabac where Z = aa. The next common byte pair is ab so let‚Äôs replace it with Y. We now have ZYdZYac where Z = aa and Y = ab. The only byte pair left is ac which appears as just one so we will not encode it. We can use recursive byte pair encoding to encode ZY as X. Our data has now transformed into XdXac where X = ZY, Y = ab, and Z = aa. It cannot be further compressed as there are no byte pairs appearing more than once. We decompress the data by performing replacements in reverse order.` <br>


### `Another Example: `
**Imagine you have a text document containing the following sentences:**<br>

"The quick brown fox jumps over the lazy dog."<br>

**You want to compress this text using BPE.** <br>

- Identify Repeated Byte Pairs:<br>
Look for repeated pairs of characters in the text. In our example, let's say the most common repeated pair is "th".<br>

- Replace Repeated Byte Pairs:<br>
Replace the most common repeated pair with a new symbol that doesn't exist in the text. Let's use `"$" for "th"`. So, our compressed text becomes:<br>

"The quick brown fox jumps over $e lazy dog."<br>

- Repeat the Process:<br>
Continue identifying and replacing repeated pairs of characters in the compressed text. Let's say the next most common pair is `"er"`. Replace it with another new symbol, let's use `"#" for "er"`. So, our compressed text becomes:<br>

"The quick brown fox jumps ov# $e lazy dog."<br>

- Repeat Until No More Replacements Can Be Made:
Keep repeating the process until no more repeated pairs of characters can be found. In our example, there are no more repeated pairs.<br>

- **Decompression:** <br>
To decompress the text, reverse the replacements. Replace the symbols with their corresponding original pairs of characters. For example, replace "$" with "th" and "#" with "er". So, our decompressed text becomes:<br>

"The quick brown fox jumps over the lazy dog."<br><br>

A variant of this is used in NLP. Let us understand the NLP version of it together. ü§ó<br>

BPE ensures that the most common words are represented in the vocabulary as a single token while the rare words are broken down into two or more subword tokens and this is in agreement with what a subword-based tokenization algorithm does.

<br>

![img](../img/optional_text_01.jpeg)
![img](../img/optional_text_02.jpeg)

<br>




