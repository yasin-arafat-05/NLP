{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# `# Stemming:`\n",
    "\n",
    "<br>\n",
    "\n",
    "For, basic see the note of 01 text preprocessing. And this is used in information retrival system. Like, Google search engine. When we search for fish, it will give the result of fishing, fishnet etc.\n",
    "Go to google and search it!!\n",
    "<br>\n",
    "\n",
    "**We will do stemming with nltk libary. For doing stemming we have to expert in lingustics. But, as a dataScientist it's not possible for us.**\n",
    "\n",
    "<br>\n",
    "\n",
    "- There is an lingustics expert named PorterStemmer. We will use it for English Language.\n",
    "- For, other language **(Without English)** there have snowballstemmer.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "stem_words(\"walk walked walking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka performacn bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and mother slow awaken to what happen in the world and under her own roof is believ and starl if i had a dozen thumb thed all be up for thi movi'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more time in the last 25 years paul lukas performacne brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and mothers slow awakening to whats happening in the world and under her own roof is believable and starling if i had a dozen thumbs thed all be up for this movie\"\n",
    "stem_words(text=text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "# `# Disadvantage of stemming and switch to lemmitization:`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "In stemming, **অতিরিক্ত স্টেমিং হয় and (ভুলভাবে শব্দ কাটা যায় )**\n",
    "\n",
    "**In the above example:** <br>\n",
    "\"probably\" → \"probabl\" (অর্থ বদলে গেল, শব্দটা ভুল হয়ে গেল)। <br>\n",
    "\"favorite\" → \"favorit\", \"movie\" → \"movi\", \"story\" → \"stori\" (অস্বাভাবিক শব্দ, ইংরেজি নিয়ম মানে না)।\n",
    "\n",
    "**But, probabl, favorit বলে english এ কোন word নেই ।**\n",
    "\n",
    "<br>\n",
    "\n",
    "That's why we use use **lemmitization:** But stemming is fast and lemmitization is slow. Cause, after doing lemmitization all the `lemma(base words)` search in English dictionary is it a valid English words or not.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/yasin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " '.',\n",
       " 'He',\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuation = \"?:!.,:;\"\n",
    "\n",
    "# tokenization:\n",
    "sentence_token = nltk.word_tokenize(sentence)\n",
    "sentence_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " 'He',\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Remove punctuation:\n",
    "for word in sentence_token:\n",
    "    if word in punctuation:\n",
    "        sentence_token.remove(word)\n",
    "sentence_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/yasin/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to download the dictionary:\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# lemmatizing:\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_token:\n",
    "    print(\"{0:20}{1:20}\".format(word,word_net_lemmatizer.lemmatize(word)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
